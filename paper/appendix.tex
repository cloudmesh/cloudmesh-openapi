\appendix

\section{Appendix}

\subsection{Setup}\label{appendix-a.---setup}

\subsubsection{Deployment}\label{a.1.-deployment}

The project is easy to replicate with our detailed instructions. First
you must install Cloudmesh OpenAPI whihch can be done by the following
steps:


\begin{verbatim}
python -m venv ~/ENV3
source ~/ENV3/bin/activate 
mkdir cm
cd cm
pip install cloudmesh-installer
cloudmesh-installer get openapi 
cms help
cms gui quick
# fill out mongo variables
# make sure autinstall is True
cms config set cloudmesh.data.mongo.MONGO_AUTOINSTALL=True
cms admin mongo install --force
# Restart a new terminal to make sure mongod 
# is in your path
cms init
\end{verbatim}
 

As a first example we like to test if the deployment works by using a
number of simple commands we execute in a terminal.

\begin{verbatim}
cd ~/cm/cloudmesh-openapi

cms openapi generate get_processor_name \
    --filename=./tests/server-cpu/cpu.py

cms openapi server start ./tests/server-cpu/cpu.yaml

curl -X GET "http://localhost:8080/cloudmesh/get_processor_name" \
     -H "accept: text/plain"
cms openapi server list

cms openapi server stop cpu
\end{verbatim}

The output will be a string containing your computer.

\TODO{how does the string look like}


As we often also need the information as a REST service, we provide in
our next example a jsonified object specification.

\begin{verbatim}
from flask import jsonify

def add(x: float, y: float) -> str:
    """
    adding float and float.
    :param x: x value
    :type x: float
    :param y: y value
    :type y: float
    :return: result
    :return type: float
    """
    result = {"result": x + y}

    return jsonify(result)
\end{verbatim}

The result will include a json string returned by the service.

\begin{verbatim}
cms openapi generate add --filename=./tests/add-json/add.py
cms openapi server start ./tests/add-json/add.yaml 
curl -X GET "http://localhost:8080/cloudmesh/add?x=1&y=2" \
     -H "accept: text/plain"
# This command returns
> {"result":3.0}
cms openapi server stop add
\end{verbatim}

These examples are used to demonstrate the ease of use as well as the
functionality for those that want to replicate our work.


\subsection{Pipiline ANOVA SVM}\label{a.2.-pipiline-anova-svm}

This example demonstrates how to deploy a simple machine learning
example onto a server using cloudmesh-openapi. The specific
implementation details that this example is based on can be found
\href{https://scikit-learn.org/stable/auto_examples/feature_selection/plot_feature_selection_pipeline.html}{here.}

The model being implemented is, in essence, an SVM with extra features
to improve the model. An SVM (support vector machine) is a supervised
learning model with associated learning algorithms used for
classification and regression analysis. This model has become one of the
most robust prediction methods widely used in problems conerning
classification and the like.

The Pipeline and ANOVA aspects are extensions to the SVM to improve the
overall model. The purpose of the pipeline is to assemble several steps
that can be cross-validated together while setting different parameters.
ANOVA on the other hand is an acronym for Analysis of Variance. It is an
omnibus test, meaning it tests for a difference overall between all
groups. In the context of an SVM, this information is useful as an SVM
mainly classifies data into separate groups.

We can now proceed as follows:

\begin{verbatim}
$ pwd
~/cm/cloudmesh-openapi

$ cms openapi generate PipelineAnovaSVM \
      --filename=./tests/Scikitlearn-experimental/sklearn_svm.py \
      --import_class --enable_upload

$ cms openapi server start ./tests/Scikitlearn-experimental/sklearn_svm.yaml
\end{verbatim}

After running these commands, we opened a web user interface. In the
user interface, we uploaded the file iris data located in
\verb|~/cm/cloudmesh-openapi/tests/|
Scikitlearn-experimental/iris.data

We then trained the model on this data set by inserting the name of the
file we uploaded \verb|iris.data|. Next, we tested the model by
clicking on make\_prediction and giving it the name of the file
iris.data and the parameters \verb|5.1|, \verb|3.5|, \verb|1.4|,
\verb|0.2|

The response we received was
\verb|Classification: ['Iris-setosa']|

Lastly, we close the server:

\begin{verbatim}
$ cms openapi server stop sklearn_svm
\end{verbatim}

This process can easily be replicated when we create more service
examples that we derive from existing sklearn examples. We benchmark
these tests while wrapping them into pytests and run them on various
cloud services.

\subsection{Eigenfaces SVM Facial
Recognition}\label{a.3.-eigenfaces-svm-facial-recognition}

Next we demonstrate how to run the Eigenfaces SVM example locally, and
then how to run its associated benchmark script.

\begin{verbatim}
$ pwd
~/cm/cloudmesh-openapi

$ git checkout benchmark # todo required until merged into main

$ cms openapi generate EigenfacesSVM \
      --filename=./tests/generator-eigenfaces-svm/eigenfaces-svm-full.py \
      --import_class --enable_upload

$ cms openapi server start \
      ./tests/generator-eigenfaces-svm/eigenfaces-svm-full.yaml
\end{verbatim}

After running these commands, we opened a web user interface at
\url{http://localhost:8080/cloudmesh/ui}. In the user interface we run
the download\_data function with the default arguments. This downloads
and extracts the labeled faces in the wild data set to the
\verb|~/scikit_learn_data/lfw_home directory|.

Next, we run the train function to train the model. The train function
performs a 50/50 train/test split on the input data, and returns
performance statistics of the trained model.

Next, we use the upload function to upload an example image using
\verb|~./tests/generator-eigenfaces-svm/example\_image.jpg|
as the function argument. This puts the example image in the
\verb|~/.cloudmesh/upload-file/| directory.

Finally, we run the predict function with the uploaded file path as an
argument,
\verb|~/.cloudmesh/upload-file/example\_image.jpg|,
and receive the classification as a response
\verb|['George W. Bush']|

Last, we close the server:

\begin{verbatim}
$ cms openapi server stop EigenfacesSVM
\end{verbatim}

Next, we benchmark these tests while wrapping them into pytests and run
them on various cloud services.

\textbf{Before continuing you must have successfully registered AWS,
Azure, and Google clouds in your yaml file and be able to boot virtual
machines on Google, AWS, and Azure. This example currently should work
on Linux and macOS}

First, we must change to a git branch that includes Azure provider
fixes, and setup our \verb|~./cloudmesh/cloudmesh.yaml| file to
replicate the parameters set for the benchmark results above. This can
be done with the commands listed in Figure \ref{fig:config}.

\TODO{NEVER USE THE WORD ABOVE to refer to previous, next or numbered items}

\begin{figure*}[htb]

\begin{verbatim}
$ cd ~/cm/cloudmesh-azure 
$ git checkout benchmark # required until changes merged to main

$ cd ~/cm/cloudmesh-openapi

$ cp ~/.cloudmesh/cloudmesh.yaml ~/.cloudmesh/cloudmesh.bak.1 # to revert reverse the cp

$ cms config set cloudmesh.cloud.azure.default.image="Canonical:0001-com-ubuntu-server-focal:20_04-lts:20.04.202006100"
$ cms config set cloudmesh.cloud.azure.default.size="Standard_D2s_v3"
$ cms config set cloudmesh.cloud.azure.credentials.AZURE_REGION="eastus"

$ cms config set cloudmesh.cloud.aws.default.image="ami-0dba2cb6798deb6d8"
$ cms config set cloudmesh.cloud.aws.default.size="m4.large"
$ cms config set cloudmesh.cloud.aws.default.username="ubuntu"
$ cms config set cloudmesh.cloud.aws.credentials.region="us-east-1"

$ cms config set cloudmesh.cloud.google.default.image="ubuntu-2004-lts"
$ cms config set cloudmesh.cloud.google.default.image_project="ubuntu-os-cloud"
$ cms config set cloudmesh.cloud.google.default.zone="us-east1-b"
$ cms config set cloudmesh.cloud.google.default.region="us-east1"
$ cms config set cloudmesh.cloud.google.default.flavor="n1-standard-2"
\end{verbatim}

  \caption{Configuration}
  \label{fig:config}
  \end{figure*}

Next, we will modify the default security group to open the flask server
port 8080 for OpenAPI service testing.

\begin{verbatim}
$ cms sec rule add openapi 8080 8080 tcp 0.0.0.0/0
$ cms sec group add default openapi for_openapi_demo
# the above two command should allow aws and azure to work
# sec group load is broken for google and it does not 
# use the default sec group, so you have to manually 
# add the openapi rule to google cloud for now
# console.cloud.google.com > VPC network > firewall > create firewall rule
# name: openapi, targets:  all instances in network, 
#       Source IP ranges: 0.0.0.0 /0, 
#       specified protocols and ports: tcp 8080 > create
\end{verbatim}

Next, we will run the benchmarking script,

\verb|./tests/generator-eigenfaces-svm/benchmark-eigenfaces.py.|

This script utilizes the Cloudmesh shell and the Bash script,

\verb|./tests/generator-eigenfaces-svm/eigenfaces-svm-full-script|,

to sequentially deploy a VM on each of the clouds, install
Cloudmesh-openapi and the example dependencies, and then us the
pytest, \\
\verb|./tests/test\_030\_generator\_eigenfaces\_svm.py|, twice to
benchmark the EigenfacesSVM service functions both locally from the
server, and from the remote client running the benchmark
script. Finally, it prints and plots performance statistics.

\begin{verbatim}
$ ./tests/generator/eigenfaces-svm/benchmark-eigenfaces.py run
\end{verbatim}

If the command line argument \verb|run| is passed to the script, then
it will start up the virtual machines on each cloud. Output and
benchmark results from each of the virtual machines will be store in the
\verb|~/.cloudmesh/eigenfaces-svm/vm\_script_output/| directory.
The benchmark results are scraped from the script outputs and stored in
the \verb|~/.cloudmesh/eigenfaces-svm/benchmark_output|
directory. If the \verb|run| argument is \textbf{not} provided, it
will only print statistics from script output already stored in the
vm\_script\_output directory.

Statistics will be printed to the command line, and graphs will be
displayed using plt.show() function calls as well as saved to the \\
\verb|./tests/generator-eigenfaces-svm/ directory|.

Next, we will run the multi-cloud benchmarking script,

\verb|.tests/generator-eigenfaces-svm/bencmark-eigenfaces-multi-cloud.py|.

This script uses the Cloudmesh shell and the Bash script,

\verb|.tests/generator-eigenfaces-svm/eigenfaces-svm-full-multi-script|,

to sequentially deploy a VM on each of the clouds, install
Cloudmesh-OpenAPI and the example dependencies, and start the AI
service. Next, it conducts HTTP requests in parallel to interact with
the services to measure the runtime for data download, training,
uploading, and prediction.

\begin{verbatim}
$ ./tests/generator/eigenfaces-svm/benchmark-eigenfaces-multi-cloud.py run
\end{verbatim}

As above the command line argument run is used to conduct actual tests,
and the absence of that argument simply computes statistics on existing
output from the \\
\verb|~/.cloudmesh/eigenfaces-svm/vm_script_output_multi/|
directory.

Statistics will be printed to the command line, and graphs will be
displayed using plt.show() function calls as well as saved to the
\verb|./tests/generator-eigenfaces-svm/ directory|.

\subsection{Using unit tests for
Benchmarking}\label{a.4.-using-unit-tests-for-benchmarking}

\TODO{This section will be expanded upon}

\begin{itemize}
\item
  Describe why we can unit tests
\item
  Describe how we access multiple clouds

\begin{verbatim}
cms set cloud=aws
# run test
cms set cloud=azure
# run test
\end{verbatim}
\item
  Describe the Benchmark class from cloudmesh in one sentence and how we
  use it

  \subsection{Basic Auth
  Example}\label{a.5.-basic-auth-example}

  Basic Auth in cloudmesh openapi can be enabled with the following flag

\begin{verbatim}
--basic_auth=<username>:<password>
\end{verbatim}

  flag. As such, this example will be an extension of a previously
  existing example. To follow with this example, navigate to the
  \verb|cloudmesh-openapi| directory.

  We will use the \verb|server-cpu| example which tells the user the
  CPU of the machine running the API.

  For this example, let's create a user with username \verb|admin| and
  password \verb|secret|.

\begin{verbatim}
cms openapi generate get_processor_name \
  --filename=./tests/server-cpu/cpu.py \
  --basic_auth=admin:secret
\end{verbatim}

  We can start the server as follows:
  
  \verb|cms openapi server start ./tests/server-cpu/cpu.yaml|

  The user will now be required to authenticate as the registered user
  in order to access the API. This can be done by specifying the Basic
  Auth credentials in the header as done
  \href{https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Authorization}{here}.
  Alternatively, the user can login via the
  \href{http://localhost:8080/cloudmesh/u}{swagger UI} when the server
  is started.
\end{itemize}

\subsection{A.6 Switching between PickleDB and
MongoDB}\label{a.6-switching-between-pickledb-and-mongodb}

The default ``out-of-the-box'' storage mechanism of cloudmesh-openapi is
Pickle. This requires no setup of the DB on the user's end.

To switch to MongoDB, the user must first change their config option as
follows:

\begin{verbatim}
cms openapi register protocol mongo
\end{verbatim}

Note that by switching to mongo, certain mongo variables need to be
filled out. Mongo may need to be installed as well. Refer to
\href{https://github.com/cloudmesh/cloudmesh-openapi/\#installation}{this}
documentation to see how this process can be done.

One may switch back to pickle with the same command:

\begin{verbatim}
cms openapi register protocol pickle
\end{verbatim}

\subsection{APPENDIX B. - Code
Location}\label{appendix-b.---code-location}

This is temporary and will in final be moved elsewhere. Its conveniently
for now placed on top so we can easier locate it at \cite{cloudmesh-openapi}.


\subsection{APPENDIX C. - Cloudmesh
Links}\label{appendix-c.---cloudmesh-links}

We added this section so the Reader can easily find some cloudmesh
related information Documentation for Cloudmesh can be found at \cite{cloudmesh-manual}:

Code for cloud mesh can be found at \cite{cloudmesh-github}.

Examples in this paper came from the cloudmesh openapi manual which is
located here \cite{cloudmesh-openapi}.

Information about cloudmesh can be found here \cite{cloudmesh-manual},
whic includes various cloudmesh installations documentation for
different OSes.

\subsection{APPENDIX D. - Plan}\label{appendix-d.---plan}

Thus far in the project we have familiarized ourselves with
Cloudmesh-Openapi by recreating example services on our local machines,
setup a git branch of the source project on which we will collaborate,
contributed to the paper's background section, and started looking for
example AI analytics, like those provided at SciKitLearn's website. We
obtained cloud service accounts from AWS, Azure, GCP, and Chameleon
Cloud, and verified Cloudmesh documentation while applying for the cloud
accounts. We registered our accounts with the Cloudmesh shell and
executed VM operations using Cloudmesh.

Moving forward, we will develop benchmark tests in the form of pytests
that replicate the AI analytic examples. We will each use Cloudmesh to
deploy these tests as an Openapi based REST service and benchmark their
performance on various cloud providers. Our benchmarks will measure
various components such as data transfer time, model train time, model
prediction time, etc. We will then consolidate and report on our
findings. Our final project will include a script that utilizes the
Cloudmesh shell to automate our benchmark tests so others can replicate
our work.

For an AI analytic benchmark test, one intesresting example to replicate
may be the faces recognition example using eigenfaces and SVMs \cite{www-skikit-learn-faces}.

Last week we created the first draft of the eigenfaces-svm example that
is found in the ``benchmark'' branch. It outputs the example and prints
benchmark information. We are making progress on manually running this
example on a cloud VM using the Cloudmesh shell, which will generate the
requirements for our final script.

This week we successfully ran the eigenfaces-svm example on Goolge
Cloud, Amazon Web Services, and Microsoft Azure. We created a script
eigenfaces-svm-script that can deploy the OpenAPI service on a fresh VM
on a cloud and run the eigenfaces-svm example. We also created the
eigenfaces-svm-full example which breaks the workflow into a functions
that download remote data, train and tests the model, provide a image
upload function, and a prediction function. We also created a pytest
that automatically run those four functions and print benchark
information.

Next week we will create a script to run the eigenfaces-svm-full example
on each cloud multiple times, and them summarize and plot benchmark
information to compare the clouds. Additionally, we will finish the
report.
